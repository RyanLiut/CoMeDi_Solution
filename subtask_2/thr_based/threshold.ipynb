{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BGUV72yz0JW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import krippendorff\n",
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "from transformers import AutoTokenizer, BertModel, AutoModelForMaskedLM\n",
    "import transformers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUvmR0Nw1A6o"
   },
   "outputs": [],
   "source": [
    "# use data for non-LLMs\n",
    "# use prompt_data for LLMs\n",
    "# YYY represents your folder name\n",
    "ROOT_DIR = \"/YYY/CoMeDi_Solution\"\n",
    "path_dev = ROOT_DIR + '/subtask_2/prompt_data/dev/'\n",
    "path_train = ROOT_DIR + '/subtask_2/prompt_data/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_dev):      \n",
    "    os.makedirs(path_dev)\n",
    "# with ZipFile('dev.zip', 'r') as dev:\n",
    "#     dev.extractall(path_dev)\n",
    "if not os.path.exists(path_train):\n",
    "    os.makedirs(path_train)\n",
    "# with ZipFile('train.zip', 'r') as train:\n",
    "#     train.extractall(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYANTo0Tz4V_"
   },
   "outputs": [],
   "source": [
    "languages = os.listdir(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_paths_train = []\n",
    "uses_file_paths_train = []\n",
    "instance_file_paths_dev = []\n",
    "uses_file_paths_dev = []\n",
    "\n",
    "for lang in languages:\n",
    "    label_file_paths_train.append(path_train + lang + '/labels.tsv')\n",
    "    uses_file_paths_train.append(path_train + lang + '/uses.tsv')\n",
    "    instance_file_paths_dev.append(path_dev + lang + '/instances.tsv')\n",
    "    uses_file_paths_dev.append(path_dev + lang + '/uses.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train labels and uses and dev instances and uses\n",
    "\n",
    "# dictionary containing input file paths\n",
    "paths = {'train_labels_list': label_file_paths_train, 'train_uses_list': uses_file_paths_train, 'dev_uses_list': uses_file_paths_dev, 'dev_instances_list': instance_file_paths_dev}\n",
    "# dictionary to store the extracted data\n",
    "data_dict = {'train_labels_list': [], 'train_uses_list': [], 'dev_uses_list': [], 'dev_instances_list': []}\n",
    "\n",
    "for save_path, path_list in paths.items():\n",
    "    for path in path_list:\n",
    "        with open(path, encoding='utf-8') as tsvfile:\n",
    "            language = path.split('/')[-2]\n",
    "            reader = csv.DictReader(tsvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "            for row in reader:\n",
    "                row['language'] = language\n",
    "                data_dict[save_path].append(row)\n",
    "\n",
    "train_labels_list = data_dict['train_labels_list']\n",
    "train_uses_list = data_dict['train_uses_list']\n",
    "dev_uses_list = data_dict['dev_uses_list']\n",
    "dev_instances_list = data_dict['dev_instances_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionaries to map identifiers to their contexts and target token indices from the train and dev uses data\n",
    "\n",
    "def create_mappings(uses_list):\n",
    "    id2context = {}\n",
    "    id2idx = {}\n",
    "    for row in uses_list:\n",
    "        identifier = row['identifier']\n",
    "        context = row['context']\n",
    "        idx = row['indices_target_token']\n",
    "        id2context[identifier] = context\n",
    "        id2idx[identifier] = idx\n",
    "    return id2context, id2idx\n",
    "\n",
    "train_id2context, train_id2idx = create_mappings(train_uses_list)\n",
    "dev_id2context, dev_id2idx = create_mappings(dev_uses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train labels and uses into a single dataframe\n",
    "# NOTE that we use the mode of judgments as the final cleaned label\n",
    "train_uses_merged= []\n",
    "for row in train_labels_list:\n",
    "    identifier1_train = row['identifier1']  \n",
    "    identifier2_train = row['identifier2']\n",
    "    \n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = train_id2context.get(identifier1_train)\n",
    "    context2 = train_id2context.get(identifier2_train)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = train_id2idx.get(identifier1_train)\n",
    "    index_target_token2 = train_id2idx.get(identifier2_train)\n",
    "            \n",
    "    lemma = row['lemma']\n",
    "    judgments = eval(row['judgments'])\n",
    "    # print(judgments) \n",
    "    median_cleaned = max(stats.mode(judgments).mode,1)\n",
    "     \n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2,'index_target_token1': index_target_token1, 'index_target_token2': index_target_token2,'identifier1': identifier1_train,'identifier2': identifier2_train,'lemma': lemma,'median_cleaned': median_cleaned,'judgments': judgments, 'language':language}\n",
    "    \n",
    "    train_uses_merged.append(data_row)\n",
    "\n",
    "df_train_uses_merged = pd.DataFrame(train_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dev instances and uses into a single dataframe\n",
    "\n",
    "dev_uses_merged = []\n",
    "for row in dev_instances_list:\n",
    "    identifier1_dev= row['identifier1']  \n",
    "    identifier2_dev = row['identifier2']\n",
    "    \n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = dev_id2context.get(identifier1_dev)\n",
    "    context2 = dev_id2context.get(identifier2_dev)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = dev_id2idx.get(identifier1_dev)\n",
    "    index_target_token2 = dev_id2idx.get(identifier2_dev)\n",
    "            \n",
    "    lemma = row['lemma']\n",
    "  \n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2,'index_target_token1': index_target_token1, 'index_target_token2': index_target_token2,'identifier1': identifier1_dev,'identifier2': identifier2_dev,'lemma': lemma, 'language':language}\n",
    "    \n",
    "    dev_uses_merged.append(data_row)\n",
    "    \n",
    "df_dev_uses_merged = pd.DataFrame(dev_uses_merged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and load the tokenizer and model for different choices\n",
    "# models can be downloaded from huggingface\n",
    "MODEL_PATH = \"/XXX/models/\" # offline model path\n",
    "model_name = \"Llama-7b-hf\" #\"Llama-7b-hf\"\n",
    "LAYER_ID = -3 # choose which layer to extract representations\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH + f\"/{model_name}\")\n",
    "if model_name in [\"bert-base-multilingual-cased\", \"bert-large-uncased\"]:\n",
    "    model = BertModel.from_pretrained(MODEL_PATH + f\"/{model_name}\").to(device)\n",
    "elif model_name in [\"xlm-roberta-base\", \"xlm-roberta-large\"]:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH + f\"/{model_name}\").to(device)\n",
    "elif model_name in [\"Llama-7b-hf\"]:\n",
    "    assert \"prompt\" in path_dev and \"prompt\" in path_train, \"Data format should adding prompts!\"\n",
    "    model = transformers.AutoModel.from_pretrained(MODEL_PATH + f\"/{model_name}\", device_map=\"auto\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate truncation indices for a sequence of tokens to ensure that the target subwords are preserved\n",
    "\n",
    "def truncation_indices(target_subword_indices: list[bool], truncation_tokens_before_target=0.5) -> tuple[int, int]:\n",
    "    max_tokens = 512\n",
    "    n_target_subtokens = target_subword_indices.count(True)\n",
    "    tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "    tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "    # get index of the first target subword\n",
    "    lindex_target = target_subword_indices.index(True)\n",
    "    # get index of the last target subword\n",
    "    rindex_target = lindex_target + n_target_subtokens\n",
    "    # starting index for truncation\n",
    "    lindex = max(lindex_target - tokens_before, 0)\n",
    "    # ending index for truncation\n",
    "    rindex = rindex_target + tokens_after\n",
    "    return lindex, rindex\n",
    "\n",
    "def get_target_token_embedding(context, index, truncation_tokens_before_target=0.5):\n",
    "    start_idx = int(str(index).strip().split(':')[0])\n",
    "    end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "    # tokenize the context with offset mapping\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=False)\n",
    "    \n",
    "    # offset mapping to provide the start and end positions of each token in the original context\n",
    "    offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "    \n",
    "    # convert input ids to tokens\n",
    "    input_ids = inputs['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # create a boolean mask for subwords within the target words span\n",
    "    subwords_bool_mask = [\n",
    "        (start <= start_idx < end) or (start < end_idx <= end) or (start_idx <= start and end <= end_idx)\n",
    "        for start, end in offset_mapping\n",
    "    ]\n",
    "\n",
    "    target_token_indices = [i for i, value in enumerate(subwords_bool_mask) if value]\n",
    "\n",
    "    if not target_token_indices:\n",
    "        print(f\"Error: Target token indices not found within the specified range for context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "   \n",
    "    # truncate input if it exceeds 512 tokens\n",
    "    if len(input_ids[0]) > 512:\n",
    "        # truncation indices based on the subwords boolean mask\n",
    "        lindex, rindex = truncation_indices(subwords_bool_mask, truncation_tokens_before_target)\n",
    "        \n",
    "        # truncate the tokens, input_ids and subwords_bool_mask within the range of truncation indices\n",
    "        tokens = tokens[lindex:rindex]\n",
    "        input_ids = input_ids[:, lindex:rindex]\n",
    "        subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "        offset_mapping = offset_mapping[lindex:rindex]\n",
    "        inputs['input_ids'] = input_ids  # update the input_ids in the inputs dictionary\n",
    "        \n",
    "        # check if truncation was successful\n",
    "        if len(input_ids[0]) > 512:\n",
    "            print(f\"Truncation failed: input seq len ({len(input_ids[0])}) exceeds the maximum token limit for context: '{context}' and index: '{index}'\")\n",
    "            return None\n",
    "    \n",
    "    # extract the subwords for the target word\n",
    "    extracted_subwords = [tokens[i] for i, value in enumerate(subwords_bool_mask) if value]\n",
    "    \n",
    "    if not extracted_subwords:\n",
    "        print(f\"Error: no subwords extracted for the target word in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs['input_ids'].to(\"cuda\"), output_hidden_states=True)  # get embeddings for the truncated input\n",
    "\n",
    "    # embeddings for all tokens in the truncated input\n",
    "    embeddings = outputs.hidden_states[LAYER_ID][0]\n",
    "\n",
    "    # embeddings for target token\n",
    "    target_embeddings = embeddings[subwords_bool_mask] \n",
    "    \n",
    "    if target_embeddings.size(0) == 0:\n",
    "        print(f\"error: no embeddings found for the target token in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "     \n",
    "    # aggregated target token embedding\n",
    "    target_embeddings_nump = target_embeddings.mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return target_embeddings_nump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anisotropy Removal\n",
    "dataframes = [df_train_uses_merged, df_dev_uses_merged]\n",
    "path = ROOT_DIR + \"/subtask_2/thr_based/\"\n",
    "file_names = [path + 'subtask2_train_embeddings.npz', path + 'subtask2_dev_embeddings.npz']\n",
    "STANDARD_type = \"std\" # techniques for anisotropy removal.\n",
    "\n",
    "for df, file_name in zip(dataframes, file_names):\n",
    "    id2embedding = {}\n",
    "    languages = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        identifier1 = row['identifier1']\n",
    "        identifier2 = row['identifier2']\n",
    "        languages.append(row['language'])\n",
    "        \n",
    "        if identifier1 not in id2embedding:\n",
    "            embedding1 = get_target_token_embedding(row['context1'], row['index_target_token1'])\n",
    "            id2embedding[identifier1] = embedding1\n",
    "        \n",
    "        if identifier2 not in id2embedding:\n",
    "            embedding2 = get_target_token_embedding(row['context2'], row['index_target_token2'])\n",
    "            id2embedding[identifier2] = embedding2\n",
    "\n",
    "    all_embeddings = np.stack([v for k,v in id2embedding.items()], axis=0)\n",
    "    mean_embeddings = np.mean(all_embeddings, axis=0)\n",
    "    std_embeddings = np.std(all_embeddings, axis=0) + 1e-6\n",
    "    if STANDARD_type == \"std\":\n",
    "        all_embeddings = (all_embeddings - mean_embeddings ) / std_embeddings\n",
    "    elif STANDARD_type == \"centering\":\n",
    "        all_embeddings = (all_embeddings - mean_embeddings )\n",
    "    elif STANDARD_type == \"PCArem\":\n",
    "        pca = PCA(n_components=1, whiten=True)\n",
    "        pca.fit(all_embeddings)\n",
    "        all_embeddings -= np.sum(all_embeddings * pca.components_[0], axis=1, keepdims=True) @ pca.components_[0].reshape(1,-1)\n",
    "    elif STANDARD_type == \"whitening\":\n",
    "        # Ref: https://kexue.fm/archives/8069\n",
    "        def compute_kernel_bias(vecs):\n",
    "            \"\"\"To calculate kernel和bias\n",
    "            vecs.shape = [num_samples, embedding_size]，\n",
    "            Final transformation: y = (x + bias).dot(kernel)\n",
    "            \"\"\"\n",
    "            mu = vecs.mean(axis=0, keepdims=True)\n",
    "            cov = np.cov(vecs.T)\n",
    "            u, s, vh = np.linalg.svd(cov)\n",
    "            W = np.dot(u, np.diag(1 / np.sqrt(s)))\n",
    "            return W, -mu\n",
    "        kernel, bias = compute_kernel_bias(all_embeddings)\n",
    "        all_embeddings = (all_embeddings + bias).dot(kernel)\n",
    "    \n",
    "    else:\n",
    "        print(\"There is no standardization!\")\n",
    "\n",
    "    id2embedding = {k:v for k,v in zip(id2embedding.keys(), all_embeddings)}\n",
    "    \n",
    "    np.savez(file_name, **id2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_train_uses_merged, df_dev_uses_merged]\n",
    "file_names = [path + 'subtask2_train_embeddings.npz', path + 'subtask2_dev_embeddings.npz']\n",
    "cosine_similarities_lists = [[], []]\n",
    "\n",
    "# iterate over the lists to compute and store cosine similarities\n",
    "for df, file_name, cosine_similarities in zip(dataframes, file_names, cosine_similarities_lists):\n",
    "    loaded_embeddings = np.load(file_name)\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            context_embedding1 = loaded_embeddings[row['identifier1']] \n",
    "            context_embedding2 = loaded_embeddings[row['identifier2']]\n",
    "            cosine_sim = cosine_similarity([context_embedding1], [context_embedding2])[0][0]\n",
    "            cosine_similarities.append(cosine_sim)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. row not there\")\n",
    "            continue\n",
    "    # add the cosine similarities to the dataFrame\n",
    "    df['cosine_similarity'] = cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_uses_merged['median_cleaned'] = df_train_uses_merged['median_cleaned'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input features for the threshold model\n",
    "\n",
    "median_cleaned_train = df_train_uses_merged['median_cleaned'].tolist()\n",
    "cosine_sim_train = df_train_uses_merged['cosine_similarity'].tolist()\n",
    "cosine_sim_dev = df_dev_uses_merged['cosine_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold model \n",
    "\n",
    "def calc_threshold(cosine_sim_train, median_cleaned_train, n=3):\n",
    "    min_sim = float(min(cosine_sim_train))\n",
    "    max_sim = float(max(cosine_sim_train))\n",
    "    delta = (max_sim - min_sim) / (n + 1)\n",
    "    # initial bins\n",
    "    bins = [min_sim + delta*(i+1) for i in range(n)]\n",
    "    \n",
    "    # loss function\n",
    "    def min_loss(bins, cos_sim, y):\n",
    "        bins = sorted([-np.inf] + list(bins) + [np.inf])\n",
    "        binned_similarities = pd.cut(cos_sim, bins=bins, labels=[1.0, 2.0, 3.0, 4.0])\n",
    "        y_pred = binned_similarities.tolist()\n",
    "        y = [float(i) for i in y]\n",
    "        data = [y, y_pred]\n",
    "        alpha = krippendorff.alpha(reliability_data=data, level_of_measurement=\"ordinal\")\n",
    "        return 1 - alpha\n",
    "    \n",
    "    # optimizing bin edges\n",
    "    result = minimize(min_loss, bins, args=(cosine_sim_train, median_cleaned_train), method='nelder-mead')\n",
    "    optimized_bins = sorted([-np.inf] + result.x.tolist() + [np.inf])\n",
    "    \n",
    "    return optimized_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate optimised bins(thresholds) per language\n",
    "\n",
    "grouped_lang_df = df_train_uses_merged.groupby('language')\n",
    "\n",
    "optimized_bins_dict = {}\n",
    "\n",
    "for language, group in grouped_lang_df:\n",
    "    median_cleaned_train = group['median_cleaned'].values.tolist()\n",
    "    cosine_sim_train = group['cosine_similarity'].values.tolist()\n",
    "    \n",
    "    optimized_bins = calc_threshold(cosine_sim_train, median_cleaned_train)\n",
    "    optimized_bins_dict[language] = optimized_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on dev based on the optimised bins\n",
    "\n",
    "predictions = []\n",
    "predictions_sim = []\n",
    "for _, row in df_dev_uses_merged.iterrows():\n",
    "    language = row['language']\n",
    "    cosine_sim_dev = row['cosine_similarity']\n",
    "    optimized_bins = optimized_bins_dict[language]\n",
    "    prediction = pd.cut([cosine_sim_dev], bins=optimized_bins, labels=[1.0, 2.0, 3.0, 4.0])\n",
    "    predictions.append(prediction[0])\n",
    "    predictions_sim.append(cosine_sim_dev)\n",
    "\n",
    "df_dev_uses_merged['prediction'] = predictions\n",
    "df_dev_uses_merged['prediction_sim'] = predictions_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output in required format for codalab\n",
    "\n",
    "out_dir = ROOT_DIR + '/subtask_2/thr_based/dev_cls_sim_results/'\n",
    "if not os.path.exists(out_dir):      \n",
    "    os.makedirs(out_dir)\n",
    "# create output in required format for codalab\n",
    "answer_df = df_dev_uses_merged[['identifier1', 'identifier2', 'prediction', 'prediction_sim', 'language']]\n",
    "answer_df = answer_df.reset_index(drop= True)\n",
    "for i in list(answer_df[\"language\"].value_counts().index):\n",
    "    df_temp = answer_df[answer_df[\"language\"]==i]\n",
    "    df_temp = df_temp.drop('language', axis=1)\n",
    "    df_temp.to_csv(out_dir +i +'.tsv',index = False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
