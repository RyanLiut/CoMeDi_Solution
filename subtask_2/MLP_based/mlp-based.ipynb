{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import csv\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"GPU : {is_available}\")\n",
    "device = torch.device(\"cuda\" if is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev = 'dev/'\n",
    "path_train = 'train/'\n",
    "\n",
    "if not os.path.exists(path_dev):       \n",
    "    os.makedirs(path_dev)\n",
    "with ZipFile('dev.zip', 'r') as dev:\n",
    "    dev.extractall(path_dev)\n",
    "if not os.path.exists(path_train):\n",
    "    os.makedirs(path_train)\n",
    "with ZipFile('train.zip', 'r') as train:\n",
    "    train.extractall(path_train)\n",
    "\n",
    "languages = os.listdir(path_train)\n",
    "\n",
    "label_file_paths_train = []\n",
    "uses_file_paths_train = []\n",
    "instance_file_paths_dev = []\n",
    "uses_file_paths_dev = []\n",
    "\n",
    "for lang in languages:\n",
    "    label_file_paths_train.append(path_train + lang + '/labels.tsv')\n",
    "    uses_file_paths_train.append(path_train + lang + '/uses.tsv')\n",
    "    instance_file_paths_dev.append(path_dev + lang + '/instances.tsv')\n",
    "    uses_file_paths_dev.append(path_dev + lang + '/uses.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train labels and uses and dev instances and uses\n",
    "\n",
    "# dictionary containing input file paths\n",
    "paths = {'train_labels_list': label_file_paths_train, 'train_uses_list': uses_file_paths_train, 'dev_uses_list': uses_file_paths_dev, 'dev_instances_list': instance_file_paths_dev}\n",
    "# dictionary to store the extracted data\n",
    "data_dict = {'train_labels_list': [], 'train_uses_list': [], 'dev_uses_list': [], 'dev_instances_list': []}\n",
    "\n",
    "for save_path, path_list in paths.items():\n",
    "    for path in path_list:\n",
    "        with open(path, encoding='utf-8') as tsvfile:\n",
    "            language = path.split('/')[1]\n",
    "            reader = csv.DictReader(tsvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "            for row in reader:\n",
    "                row['language'] = language\n",
    "                data_dict[save_path].append(row)\n",
    "\n",
    "train_labels_list = data_dict['train_labels_list']\n",
    "train_uses_list = data_dict['train_uses_list']\n",
    "dev_uses_list = data_dict['dev_uses_list']\n",
    "dev_instances_list = data_dict['dev_instances_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionaries to map identifiers to their contexts and target token indices from the train and dev uses data\n",
    "\n",
    "def create_mappings(uses_list):\n",
    "    id2context = {}\n",
    "    id2idx = {}\n",
    "    for row in uses_list:\n",
    "        identifier = row['identifier']\n",
    "        context = row['context']\n",
    "        idx = row['indices_target_token']\n",
    "        id2context[identifier] = context\n",
    "        id2idx[identifier] = idx\n",
    "    return id2context, id2idx\n",
    "\n",
    "train_id2context, train_id2idx = create_mappings(train_uses_list)\n",
    "dev_id2context, dev_id2idx = create_mappings(dev_uses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train labels and uses into a single dataframe\n",
    "\n",
    "train_uses_merged= []\n",
    "for row in train_labels_list:\n",
    "    identifier1_train = row['identifier1']  \n",
    "    identifier2_train = row['identifier2']\n",
    "    \n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = train_id2context.get(identifier1_train)\n",
    "    context2 = train_id2context.get(identifier2_train)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = train_id2idx.get(identifier1_train)\n",
    "    index_target_token2 = train_id2idx.get(identifier2_train)\n",
    "            \n",
    "    lemma = row['lemma']\n",
    "    mean_disagreement = row['mean_disagreement_cleaned']\n",
    "    judgments = row['judgments']  \n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2,'index_target_token1': index_target_token1, 'index_target_token2': index_target_token2,'identifier1': identifier1_train,'identifier2': identifier2_train,'lemma': lemma,'mean_disagreement_cleaned': mean_disagreement,'judgments': judgments, 'language':language}\n",
    "    \n",
    "    train_uses_merged.append(data_row)\n",
    "\n",
    "df_train_uses_merged = pd.DataFrame(train_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dev instances and uses into a single dataframe\n",
    "\n",
    "dev_uses_merged = []\n",
    "for row in dev_instances_list:\n",
    "    identifier1_dev= row['identifier1']  \n",
    "    identifier2_dev = row['identifier2']\n",
    "    \n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = dev_id2context.get(identifier1_dev)\n",
    "    context2 = dev_id2context.get(identifier2_dev)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = dev_id2idx.get(identifier1_dev)\n",
    "    index_target_token2 = dev_id2idx.get(identifier2_dev)\n",
    "            \n",
    "    lemma = row['lemma']\n",
    "  \n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2,'index_target_token1': index_target_token1, 'index_target_token2': index_target_token2,'identifier1': identifier1_dev,'identifier2': identifier2_dev,'lemma': lemma, 'language':language}\n",
    "    \n",
    "    dev_uses_merged.append(data_row)\n",
    "    \n",
    "df_dev_uses_merged = pd.DataFrame(dev_uses_merged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and load the tokenizer and model for XLM-RoBERTa. you need to change the local model address\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\").to(device)\n",
    "\n",
    "# define and load the tokenizer and model for mBERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-mult\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-mult\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncation_indices(target_subword_indices: list[bool], truncation_tokens_before_target=0.5) -> tuple[int, int]:\n",
    "    max_tokens = 512\n",
    "    n_target_subtokens = target_subword_indices.count(True)\n",
    "    tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "    tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "    # get index of the first target subword\n",
    "    lindex_target = target_subword_indices.index(True)\n",
    "    # get index of the last target subword\n",
    "    rindex_target = lindex_target + n_target_subtokens\n",
    "    # starting index for truncation\n",
    "    lindex = max(lindex_target - tokens_before, 0)\n",
    "    # ending index for truncation\n",
    "    rindex = rindex_target + tokens_after\n",
    "    return lindex, rindex\n",
    "\n",
    "\n",
    "def get_target_token_embedding(context, index, truncation_tokens_before_target=0.5):\n",
    "    start_idx = int(str(index).strip().split(':')[0])\n",
    "    end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "    # tokenize the context with offset mapping\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=False)\n",
    "    \n",
    "    # offset mapping to provide the start and end positions of each token in the original context\n",
    "    offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "    \n",
    "    # convert input ids to tokens\n",
    "    input_ids = inputs['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # create a boolean mask for subwords within the target words span\n",
    "    subwords_bool_mask = [\n",
    "        (start <= start_idx < end) or (start < end_idx <= end) or (start_idx <= start and end <= end_idx)\n",
    "        for start, end in offset_mapping\n",
    "    ]\n",
    "\n",
    "    target_token_indices = [i for i, value in enumerate(subwords_bool_mask) if value]\n",
    "\n",
    "    if not target_token_indices:\n",
    "        print(f\"Error: Target token indices not found within the specified range for context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "   \n",
    "    # truncate input if it exceeds 512 tokens\n",
    "    if len(input_ids[0]) > 512:\n",
    "        # truncation indices based on the subwords boolean mask\n",
    "        lindex, rindex = truncation_indices(subwords_bool_mask, truncation_tokens_before_target)\n",
    "        \n",
    "        # truncate the tokens, input_ids and subwords_bool_mask within the range of truncation indices\n",
    "        tokens = tokens[lindex:rindex]\n",
    "        input_ids = input_ids[:, lindex:rindex]\n",
    "        subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "        offset_mapping = offset_mapping[lindex:rindex]\n",
    "        inputs['input_ids'] = input_ids  # update the input_ids in the inputs dictionary\n",
    "        \n",
    "        # check if truncation was successful\n",
    "        if len(input_ids[0]) > 512:\n",
    "            print(f\"Truncation failed: input sequence length ({len(input_ids[0])}) exceeds the maximum token limit for context: '{context}' and index: '{index}'\")\n",
    "            return None\n",
    "    \n",
    "    # extract the subwords for the target word\n",
    "    extracted_subwords = [tokens[i] for i, value in enumerate(subwords_bool_mask) if value]\n",
    "    \n",
    "    if not extracted_subwords:\n",
    "        print(f\"Error: no subwords extracted for the target word in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs['input_ids'].to(device))  # get embeddings for the truncated input\n",
    "\n",
    "    # embeddings for all tokens in the truncated input\n",
    "    embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "    # embeddings for target token\n",
    "    target_embeddings = embeddings[subwords_bool_mask] \n",
    "    \n",
    "    if target_embeddings.size(0) == 0:\n",
    "        print(f\"error: no embeddings found for the target token in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "     \n",
    "    # aggregated target token embedding\n",
    "    target_embeddings_nump = target_embeddings.mean(dim=0).cpu().numpy()\n",
    "    # target_embeddings_nump = np.concatenate(( target_embeddings_nump, embeddings[0].cpu().numpy()))\n",
    "\n",
    "    return target_embeddings_nump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_train_uses_merged, df_dev_uses_merged]\n",
    "file_names = ['subtask2_train_embeddings.npz', 'subtask2_dev_embeddings.npz']\n",
    "\n",
    "# getting target token embeddings for contexts in train and dev \n",
    "for df, file_name in zip(dataframes, file_names):\n",
    "    id2embedding = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        identifier1 = row['identifier1']\n",
    "        identifier2 = row['identifier2']\n",
    "        \n",
    "        if identifier1 not in id2embedding:\n",
    "            embedding1 = get_target_token_embedding(row['context1'], row['index_target_token1'])\n",
    "            id2embedding[identifier1] = embedding1\n",
    "        \n",
    "        if identifier2 not in id2embedding:\n",
    "            embedding2 = get_target_token_embedding(row['context2'], row['index_target_token2'])\n",
    "            id2embedding[identifier2] = embedding2\n",
    "\n",
    "    # store embeddings in a .npz file using identifiers as keys\n",
    "    np.savez(file_name, **id2embedding)\n",
    "    \n",
    "    \n",
    "dataframes = [df_train_uses_merged, df_dev_uses_merged]\n",
    "file_names = ['subtask2_train_embeddings.npz', 'subtask2_dev_embeddings.npz']\n",
    "embeddings_lists = [[], []]\n",
    "\n",
    "# retrieve the context embeddings using the identifiers from the dataframe\n",
    "for df, file_name, embeddings in zip(dataframes, file_names, embeddings_lists):\n",
    "    loaded_embeddings = np.load(file_name)\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            context_embedding1 = loaded_embeddings[row['identifier1']]\n",
    "            context_embedding2 = loaded_embeddings[row['identifier2']]\n",
    "            # concatenate the embeddings to form a single feature vector\n",
    "            concatenated_emb = np.concatenate((context_embedding1, context_embedding2))\n",
    "            embeddings.append(concatenated_emb)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Identifier not found in embeddings file.\")\n",
    "            continue\n",
    "\n",
    "# convert the lists of feature vectors to numpy arrays (feature matrices)\n",
    "train_embeddings = np.array(embeddings_lists[0])\n",
    "dev_embeddings = np.array(embeddings_lists[1])\n",
    "\n",
    "\n",
    "df_train_uses_merged['mean_disagreement_cleaned'] = df_train_uses_merged['mean_disagreement_cleaned'].astype(float)\n",
    "\n",
    "\n",
    "# define features and target variable for the model\n",
    "\n",
    "X_train = train_embeddings\n",
    "y_train = df_train_uses_merged['mean_disagreement_cleaned'].values\n",
    "X_dev = dev_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random number seed\n",
    "\n",
    "def seedConfig(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, all_embedding, all_label):\n",
    "        self.all_embedding = all_embedding\n",
    "        self.all_label = all_label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        embedding = self.all_embedding[index]\n",
    "        label = self.all_label[index]\n",
    "        return embedding, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP model\n",
    "\n",
    "class MModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MModel, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.regression = nn.Linear(768*2, 1)\n",
    "    \n",
    "    def forward(self, embeddings):        \n",
    "        \n",
    "        embeddings = self.dropout(embeddings)\n",
    "        logits = self.regression(embeddings)        \n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "\n",
    "def train_loop(train_dataloader, model, loss_fn, optimizer, epoch):\n",
    "    total_loss_train = 0\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(train_dataloader, colour='green'):    \n",
    "        labels = labels.to(device)\n",
    "        logits = model(torch.Tensor(inputs).to(device))\n",
    "        loss = loss_fn(logits.float(), labels.float())\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        lr_scheduler.step()  \n",
    "                \n",
    "    print(f'Epoch {epoch}    Loss: {total_loss_train / size:.5f}', flush=True)\n",
    "    return total_loss_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set random number seed\n",
    "    seed = 1\n",
    "    seedConfig(seed)\n",
    "\n",
    "    # the hyperparameters required for training\n",
    "    learning_rate = 1e-2\n",
    "    epoch_num = 200\n",
    "    batch_size = 32\n",
    "\n",
    "    # instantiate model\n",
    "    model = MModel().to(device)\n",
    "\n",
    "    # organize the dataset\n",
    "    train_dataset = TextDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    # define loss function, optimizer and lr_scheduler\n",
    "    loss_fn = nn.MSELoss()\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in list(model.named_parameters()) if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in list(model.named_parameters()) if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=epoch_num*len(train_dataloader)*0.1, num_training_steps=epoch_num*len(train_dataloader))\n",
    "\n",
    "    # start model training\n",
    "    for t in range(epoch_num):\n",
    "        print(f'Epoch {t + 1}/{epoch_num}\\n-----------------------', flush=True)\n",
    "        loss = train_loop(train_dataloader, model, loss_fn, optimizer, t+1)\n",
    "\n",
    "    # predicting on the dev data per language\n",
    "    for language, group in df_dev_uses_merged.groupby('language'):\n",
    "        dev_indices = group.index\n",
    "        X_dev = dev_embeddings[dev_indices]\n",
    "        \n",
    "        # predict using the fitted model\n",
    "        y_pred = []\n",
    "        model.eval()\n",
    "        for dev_embedding in X_dev:\n",
    "            y_pred.append(model(torch.tensor(dev_embedding).to(device)).cpu().detach().numpy())\n",
    "        \n",
    "        # add predictions to the dataframe\n",
    "        df_dev_uses_merged.loc[dev_indices, 'prediction'] = y_pred\n",
    "        \n",
    "        \n",
    "    # create answer file in required format for codalab\n",
    "    out_dir = 'answer/'\n",
    "    if not os.path.exists(out_dir):\n",
    "            os.mkdir(out_dir)\n",
    "    answer_df = df_dev_uses_merged[['identifier1', 'identifier2', 'prediction', 'language']]\n",
    "    answer_df = answer_df.reset_index(drop= True)\n",
    "    for i in list(answer_df[\"language\"].value_counts().index):\n",
    "        df_temp = answer_df[answer_df[\"language\"]==i]\n",
    "        df_temp = df_temp.drop('language', axis=1)\n",
    "        df_temp.to_csv('answer/' +i +'.tsv',index = False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "    with ZipFile('answer.zip', 'w') as zipf:\n",
    "        for root, _, files in os.walk(out_dir):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), arcname=file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
