{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5BGUV72yz0JW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import krippendorff\n",
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "from transformers import AutoTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vUvmR0Nw1A6o"
   },
   "outputs": [],
   "source": [
    "path_dev = '../data/dev/'\n",
    "path_train = '../data/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_dev):      \n",
    "    os.makedirs(path_dev)\n",
    "with ZipFile('dev.zip', 'r') as dev:\n",
    "    dev.extractall(path_dev)\n",
    "if not os.path.exists(path_train):\n",
    "    os.makedirs(path_train)\n",
    "with ZipFile('train.zip', 'r') as train:\n",
    "    train.extractall(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XYANTo0Tz4V_"
   },
   "outputs": [],
   "source": [
    "languages = os.listdir(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file_paths_train = []\n",
    "uses_file_paths_train = []\n",
    "instance_file_paths_dev = []\n",
    "uses_file_paths_dev = []\n",
    "\n",
    "for lang in languages:\n",
    "    label_file_paths_train.append(path_train + lang + '/labels.tsv')\n",
    "    uses_file_paths_train.append(path_train + lang + '/uses.tsv')\n",
    "    instance_file_paths_dev.append(path_dev + lang + '/instances.tsv')\n",
    "    uses_file_paths_dev.append(path_dev + lang + '/uses.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train labels and uses and dev instances and uses\n",
    "\n",
    "# dictionary containing input file paths\n",
    "paths = {'train_labels_list': label_file_paths_train, 'train_uses_list': uses_file_paths_train, 'dev_uses_list': uses_file_paths_dev, 'dev_instances_list': instance_file_paths_dev}\n",
    "# dictionary to store the extracted data\n",
    "data_dict = {'train_labels_list': [], 'train_uses_list': [], 'dev_uses_list': [], 'dev_instances_list': []}\n",
    "\n",
    "for save_path, path_list in paths.items():\n",
    "    for path in path_list:\n",
    "        with open(path, encoding='utf-8') as tsvfile:\n",
    "            language = path.split('/')[1]\n",
    "            reader = csv.DictReader(tsvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "            for row in reader:\n",
    "                row['language'] = language\n",
    "                data_dict[save_path].append(row)\n",
    "\n",
    "train_labels_list = data_dict['train_labels_list']\n",
    "train_uses_list = data_dict['train_uses_list']\n",
    "dev_uses_list = data_dict['dev_uses_list']\n",
    "dev_instances_list = data_dict['dev_instances_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionaries to map identifiers to their contexts and target token indices from the train and dev uses data\n",
    "\n",
    "def create_mappings(uses_list):\n",
    "    id2context = {}\n",
    "    id2idx = {}\n",
    "    for row in uses_list:\n",
    "        identifier = row['identifier']\n",
    "        context = row['context']\n",
    "        idx = row['indices_target_token']\n",
    "        id2context[identifier] = context\n",
    "        id2idx[identifier] = idx\n",
    "    return id2context, id2idx\n",
    "\n",
    "train_id2context, train_id2idx = create_mappings(train_uses_list)\n",
    "dev_id2context, dev_id2idx = create_mappings(dev_uses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train labels and uses into a single dataframe\n",
    "\n",
    "train_uses_merged= []\n",
    "for row in train_labels_list:\n",
    "    identifier1_train = row['identifier1']  \n",
    "    identifier2_train = row['identifier2']\n",
    "    \n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = train_id2context.get(identifier1_train)\n",
    "    context2 = train_id2context.get(identifier2_train)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = train_id2idx.get(identifier1_train)\n",
    "    index_target_token2 = train_id2idx.get(identifier2_train)\n",
    "            \n",
    "    lemma = row['lemma']\n",
    "    median_cleaned = row['median_cleaned']\n",
    "    judgments = row['judgments']  \n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2,'index_target_token1': index_target_token1, 'index_target_token2': index_target_token2,'identifier1': identifier1_train,'identifier2': identifier2_train,'lemma': lemma,'median_cleaned': median_cleaned,'judgments': judgments, 'language':language}\n",
    "    \n",
    "    train_uses_merged.append(data_row)\n",
    "\n",
    "df_train_uses_merged = pd.DataFrame(train_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dev instances and uses into a single dataframe\n",
    "\n",
    "dev_uses_merged = []\n",
    "for row in dev_instances_list:\n",
    "    identifier1_dev= row['identifier1']  \n",
    "    identifier2_dev = row['identifier2']\n",
    "    \n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = dev_id2context.get(identifier1_dev)\n",
    "    context2 = dev_id2context.get(identifier2_dev)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = dev_id2idx.get(identifier1_dev)\n",
    "    index_target_token2 = dev_id2idx.get(identifier2_dev)\n",
    "            \n",
    "    lemma = row['lemma']\n",
    "  \n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2,'index_target_token1': index_target_token1, 'index_target_token2': index_target_token2,'identifier1': identifier1_dev,'identifier2': identifier2_dev,'lemma': lemma, 'language':language}\n",
    "    \n",
    "    dev_uses_merged.append(data_row)\n",
    "    \n",
    "df_dev_uses_merged = pd.DataFrame(dev_uses_merged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ver/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define and load the tokenizer and model for XLM-RoBERTa\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate truncation indices for a sequence of tokens to ensure that the target subwords are preserved\n",
    "\n",
    "def truncation_indices(target_subword_indices: list[bool], truncation_tokens_before_target=0.5) -> tuple[int, int]:\n",
    "    max_tokens = 512\n",
    "    n_target_subtokens = target_subword_indices.count(True)\n",
    "    tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "    tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "    # get index of the first target subword\n",
    "    lindex_target = target_subword_indices.index(True)\n",
    "    # get index of the last target subword\n",
    "    rindex_target = lindex_target + n_target_subtokens\n",
    "    # starting index for truncation\n",
    "    lindex = max(lindex_target - tokens_before, 0)\n",
    "    # ending index for truncation\n",
    "    rindex = rindex_target + tokens_after\n",
    "    return lindex, rindex\n",
    "\n",
    "def get_target_token_embedding(context, index, truncation_tokens_before_target=0.5):\n",
    "    start_idx = int(str(index).strip().split(':')[0])\n",
    "    end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "    # tokenize the context with offset mapping\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=False)\n",
    "    \n",
    "    # offset mapping to provide the start and end positions of each token in the original context\n",
    "    offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "    \n",
    "    # convert input ids to tokens\n",
    "    input_ids = inputs['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # create a boolean mask for subwords within the target words span\n",
    "    subwords_bool_mask = [\n",
    "        (start <= start_idx < end) or (start < end_idx <= end) or (start_idx <= start and end <= end_idx)\n",
    "        for start, end in offset_mapping\n",
    "    ]\n",
    "\n",
    "    target_token_indices = [i for i, value in enumerate(subwords_bool_mask) if value]\n",
    "\n",
    "    if not target_token_indices:\n",
    "        print(f\"Error: Target token indices not found within the specified range for context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "   \n",
    "    # truncate input if it exceeds 512 tokens\n",
    "    if len(input_ids[0]) > 512:\n",
    "        # truncation indices based on the subwords boolean mask\n",
    "        lindex, rindex = truncation_indices(subwords_bool_mask, truncation_tokens_before_target)\n",
    "        \n",
    "        # truncate the tokens, input_ids and subwords_bool_mask within the range of truncation indices\n",
    "        tokens = tokens[lindex:rindex]\n",
    "        input_ids = input_ids[:, lindex:rindex]\n",
    "        subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "        offset_mapping = offset_mapping[lindex:rindex]\n",
    "        inputs['input_ids'] = input_ids  # update the input_ids in the inputs dictionary\n",
    "        \n",
    "        # check if truncation was successful\n",
    "        if len(input_ids[0]) > 512:\n",
    "            print(f\"Truncation failed: input seq len ({len(input_ids[0])}) exceeds the maximum token limit for context: '{context}' and index: '{index}'\")\n",
    "            return None\n",
    "    \n",
    "    # extract the subwords for the target word\n",
    "    extracted_subwords = [tokens[i] for i, value in enumerate(subwords_bool_mask) if value]\n",
    "    \n",
    "    if not extracted_subwords:\n",
    "        print(f\"Error: no subwords extracted for the target word in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs['input_ids'])  # get embeddings for the truncated input\n",
    "\n",
    "    # embeddings for all tokens in the truncated input\n",
    "    embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "    # embeddings for target token\n",
    "    target_embeddings = embeddings[subwords_bool_mask] \n",
    "    \n",
    "    if target_embeddings.size(0) == 0:\n",
    "        print(f\"error: no embeddings found for the target token in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "     \n",
    "    # aggregated target token embedding\n",
    "    target_embeddings_nump = target_embeddings.mean(dim=0).numpy()\n",
    "\n",
    "    return target_embeddings_nump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataframes = [df_train_uses_merged, df_dev_uses_merged]\n",
    "file_names = ['subtask1_train_embeddings.npz', 'subtask1_dev_embeddings.npz']\n",
    "\n",
    "# getting target token embeddings for contexts in train and dev \n",
    "for df, file_name in zip(dataframes, file_names):\n",
    "    id2embedding = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        identifier1 = row['identifier1']\n",
    "        identifier2 = row['identifier2']\n",
    "        \n",
    "        if identifier1 not in id2embedding:\n",
    "            embedding1 = get_target_token_embedding(row['context1'], row['index_target_token1'])\n",
    "            id2embedding[identifier1] = embedding1\n",
    "        \n",
    "        if identifier2 not in id2embedding:\n",
    "            embedding2 = get_target_token_embedding(row['context2'], row['index_target_token2'])\n",
    "            id2embedding[identifier2] = embedding2\n",
    "\n",
    "    # store embeddings in a .npz file using identifiers as keys\n",
    "    np.savez(file_name, **id2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_train_uses_merged, df_dev_uses_merged]\n",
    "file_names = ['subtask1_train_embeddings.npz', 'subtask1_dev_embeddings.npz']\n",
    "cosine_similarities_lists = [[], []]\n",
    "\n",
    "# iterate over the lists to compute and store cosine similarities\n",
    "for df, file_name, cosine_similarities in zip(dataframes, file_names, cosine_similarities_lists):\n",
    "    loaded_embeddings = np.load(file_name)\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            context_embedding1 = loaded_embeddings[row['identifier1']] \n",
    "            context_embedding2 = loaded_embeddings[row['identifier2']]\n",
    "            cosine_sim = cosine_similarity([context_embedding1], [context_embedding2])[0][0]\n",
    "            cosine_similarities.append(cosine_sim)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. row not there\")\n",
    "            continue\n",
    "    # add the cosine similarities to the dataFrame\n",
    "    df['cosine_similarity'] = cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_uses_merged['median_cleaned'] = df_train_uses_merged['median_cleaned'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input features for the threshold model\n",
    "\n",
    "median_cleaned_train = df_train_uses_merged['median_cleaned'].tolist()\n",
    "cosine_sim_train = df_train_uses_merged['cosine_similarity'].tolist()\n",
    "cosine_sim_dev = df_dev_uses_merged['cosine_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold model \n",
    "\n",
    "def calc_threshold(cosine_sim_train, median_cleaned_train, n=3):\n",
    "    min_sim = float(min(cosine_sim_train))\n",
    "    max_sim = float(max(cosine_sim_train))\n",
    "    delta = (max_sim - min_sim) / (n + 1)\n",
    "    # initial bins\n",
    "    bins = [min_sim + delta*(i+1) for i in range(n)]\n",
    "    \n",
    "    # loss function\n",
    "    def min_loss(bins, cos_sim, y):\n",
    "        bins = sorted([-np.inf] + list(bins) + [np.inf])\n",
    "        binned_similarities = pd.cut(cos_sim, bins=bins, labels=[1.0, 2.0, 3.0, 4.0])\n",
    "        y_pred = binned_similarities.tolist()\n",
    "        y = [float(i) for i in y]\n",
    "        data = [y, y_pred]\n",
    "        alpha = krippendorff.alpha(reliability_data=data, level_of_measurement=\"ordinal\")\n",
    "        return 1 - alpha\n",
    "    \n",
    "    # optimizing bin edges\n",
    "    result = minimize(min_loss, bins, args=(cosine_sim_train, median_cleaned_train), method='nelder-mead')\n",
    "    optimized_bins = sorted([-np.inf] + result.x.tolist() + [np.inf])\n",
    "    \n",
    "    return optimized_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate optimised bins(thresholds) per language\n",
    "\n",
    "grouped_lang_df = df_train_uses_merged.groupby('language')\n",
    "\n",
    "optimized_bins_dict = {}\n",
    "\n",
    "for language, group in grouped_lang_df:\n",
    "    median_cleaned_train = group['median_cleaned'].values.tolist()\n",
    "    cosine_sim_train = group['cosine_similarity'].values.tolist()\n",
    "    \n",
    "    optimized_bins = calc_threshold(cosine_sim_train, median_cleaned_train)\n",
    "    optimized_bins_dict[language] = optimized_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions on dev based on the optimised bins\n",
    "\n",
    "predictions = []\n",
    "for _, row in df_dev_uses_merged.iterrows():\n",
    "    language = row['language']\n",
    "    cosine_sim_dev = row['cosine_similarity']\n",
    "    optimized_bins = optimized_bins_dict[language]\n",
    "    prediction = pd.cut([cosine_sim_dev], bins=optimized_bins, labels=[1.0, 2.0, 3.0, 4.0])\n",
    "    predictions.append(prediction[0])\n",
    "\n",
    "df_dev_uses_merged['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output in required format for codalab\n",
    "\n",
    "out_dir = 'answer/'\n",
    "if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "answer_df = df_dev_uses_merged[['identifier1', 'identifier2', 'prediction', 'language']]\n",
    "answer_df = answer_df.reset_index(drop= True)\n",
    "for i in list(answer_df[\"language\"].value_counts().index):\n",
    "    df_temp = answer_df[answer_df[\"language\"]==i]\n",
    "    df_temp = df_temp.drop('language', axis=1)\n",
    "    df_temp.to_csv('answer/' +i +'.tsv',index = False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "with ZipFile('answer.zip', 'w') as zipf:\n",
    "    for root, _, files in os.walk(out_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), arcname=file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
