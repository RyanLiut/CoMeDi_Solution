{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import sys\n",
    "import subprocess\n",
    "import krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"GPU : {is_available}\")\n",
    "device = torch.device(\"cuda\" if is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "path_dev = 'dev/'\n",
    "path_train = 'train/'\n",
    "path_test = 'test/'\n",
    "\n",
    "if not os.path.exists(path_dev):\n",
    "    os.makedirs(path_dev)\n",
    "with ZipFile('dev.zip', 'r') as dev:\n",
    "    dev.extractall(path_dev)\n",
    "if not os.path.exists(path_train):\n",
    "    os.makedirs(path_train)\n",
    "with ZipFile('train.zip', 'r') as train:\n",
    "    train.extractall(path_train)\n",
    "if not os.path.exists(path_test):\n",
    "    os.makedirs(path_test)\n",
    "with ZipFile('test.zip', 'r') as test:\n",
    "    test.extractall(path_test)\n",
    "\n",
    "languages = os.listdir(path_train)\n",
    "\n",
    "label_file_paths_train = []\n",
    "uses_file_paths_train = []\n",
    "instance_file_paths_dev = []\n",
    "uses_file_paths_dev = []\n",
    "instance_file_paths_test = []\n",
    "uses_file_paths_test = []\n",
    "\n",
    "for lang in languages:\n",
    "    label_file_paths_train.append(path_train + lang + '/labels.tsv')\n",
    "    uses_file_paths_train.append(path_train + lang + '/uses.tsv')\n",
    "    instance_file_paths_dev.append(path_dev + lang + '/instances.tsv')\n",
    "    uses_file_paths_dev.append(path_dev + lang + '/uses.tsv')\n",
    "    instance_file_paths_test.append(path_test + lang + '/instances.tsv')\n",
    "    uses_file_paths_test.append(path_test + lang + '/uses.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train labels and uses and dev instances and uses (test instances and uses)\n",
    "\n",
    "# dictionary containing input file paths\n",
    "paths = {'train_labels_list': label_file_paths_train, 'train_uses_list': uses_file_paths_train,\n",
    "         'dev_uses_list': uses_file_paths_dev, 'dev_instances_list': instance_file_paths_dev,\n",
    "         'test_uses_list': uses_file_paths_test, 'test_instances_list': instance_file_paths_test}\n",
    "# dictionary to store the extracted data\n",
    "data_dict = {'train_labels_list': [], 'train_uses_list': [],\n",
    "             'dev_uses_list': [], 'dev_instances_list': [],\n",
    "             'test_uses_list': [], 'test_instances_list': []}\n",
    "\n",
    "for save_path, path_list in paths.items():\n",
    "    for path in path_list:\n",
    "        with open(path, encoding='utf-8') as tsvfile:\n",
    "            language = path.split('/')[1]\n",
    "            reader = csv.DictReader(tsvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "            for row in reader:\n",
    "                row['language'] = language\n",
    "                data_dict[save_path].append(row)\n",
    "\n",
    "train_labels_list = data_dict['train_labels_list']\n",
    "train_uses_list = data_dict['train_uses_list']\n",
    "dev_uses_list = data_dict['dev_uses_list']\n",
    "dev_instances_list = data_dict['dev_instances_list']\n",
    "test_uses_list = data_dict['test_uses_list']\n",
    "test_instances_list = data_dict['test_instances_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionaries to map identifiers to their contexts and target token indices from the train and dev uses data (test uses data)\n",
    "def create_mappings(uses_list):\n",
    "    id2context = {}\n",
    "    id2idx = {}\n",
    "    for row in uses_list:\n",
    "        identifier = row['identifier']\n",
    "        context = row['context']\n",
    "        idx = row['indices_target_token']\n",
    "        id2context[identifier] = context\n",
    "        id2idx[identifier] = idx\n",
    "    return id2context, id2idx\n",
    "\n",
    "train_id2context, train_id2idx = create_mappings(train_uses_list)\n",
    "dev_id2context, dev_id2idx = create_mappings(dev_uses_list)\n",
    "test_id2context, test_id2idx = create_mappings(test_uses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train labels and uses into a single dataframe\n",
    "train_uses_merged = []\n",
    "for row in train_labels_list:\n",
    "    identifier1_train = row['identifier1']\n",
    "    identifier2_train = row['identifier2']\n",
    "\n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = train_id2context.get(identifier1_train)\n",
    "    context2 = train_id2context.get(identifier2_train)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = train_id2idx.get(identifier1_train)\n",
    "    index_target_token2 = train_id2idx.get(identifier2_train)\n",
    "\n",
    "    lemma = row['lemma']\n",
    "    median_cleaned = row['median_cleaned']\n",
    "    judgments = row['judgments']\n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2, 'index_target_token1': index_target_token1,\n",
    "                'index_target_token2': index_target_token2, 'identifier1': identifier1_train,\n",
    "                'identifier2': identifier2_train, 'lemma': lemma, 'median_cleaned': median_cleaned,\n",
    "                'judgments': judgments, 'language': language}\n",
    "\n",
    "    train_uses_merged.append(data_row)\n",
    "\n",
    "df_train_uses_merged = pd.DataFrame(train_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dev instances and uses into a single dataframe\n",
    "dev_uses_merged = []\n",
    "for row in dev_instances_list:\n",
    "    identifier1_dev = row['identifier1']\n",
    "    identifier2_dev = row['identifier2']\n",
    "\n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = dev_id2context.get(identifier1_dev)\n",
    "    context2 = dev_id2context.get(identifier2_dev)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = dev_id2idx.get(identifier1_dev)\n",
    "    index_target_token2 = dev_id2idx.get(identifier2_dev)\n",
    "\n",
    "    lemma = row['lemma']\n",
    "\n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2, 'index_target_token1': index_target_token1,\n",
    "                'index_target_token2': index_target_token2, 'identifier1': identifier1_dev, 'median_cleaned': median_cleaned,\n",
    "                'identifier2': identifier2_dev, 'lemma': lemma, 'language': language}\n",
    "\n",
    "    dev_uses_merged.append(data_row)\n",
    "\n",
    "    df_dev_uses_merged = pd.DataFrame(dev_uses_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging test instances and uses into a single dataframe\n",
    "test_uses_merged = []\n",
    "for row in test_instances_list:\n",
    "    identifier1_test = row['identifier1']\n",
    "    identifier2_test = row['identifier2']\n",
    "\n",
    "    # use id2context dictionary to get the corresponding context for each identifier\n",
    "    context1 = test_id2context.get(identifier1_test)\n",
    "    context2 = test_id2context.get(identifier2_test)\n",
    "\n",
    "    # use id2idx dictionary to get the corresponding target token index for each identifier\n",
    "    index_target_token1 = test_id2idx.get(identifier1_test)\n",
    "    index_target_token2 = test_id2idx.get(identifier2_test)\n",
    "\n",
    "    lemma = row['lemma']\n",
    "\n",
    "    language = row['language']\n",
    "    data_row = {'context1': context1, 'context2': context2, 'index_target_token1': index_target_token1,\n",
    "                'index_target_token2': index_target_token2, 'identifier1': identifier1_test, 'median_cleaned': median_cleaned,\n",
    "                'identifier2': identifier2_test, 'lemma': lemma, 'language': language}\n",
    "\n",
    "    test_uses_merged.append(data_row)\n",
    "\n",
    "df_test_uses_merged = pd.DataFrame(test_uses_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and load the tokenizer and model for XLM-RoBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\").to(device)\n",
    "\n",
    "# define and load the tokenizer and model for mBERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-mult\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-mult\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate truncation indices for a sequence of tokens to ensure that the target subwords are preserved\n",
    "# print('calculate truncation indices for a sequence of tokens to ensure that the target subwords are preserved')\n",
    "def truncation_indices(target_subword_indices: list[bool], truncation_tokens_before_target=0.5) -> tuple[int, int]:\n",
    "    max_tokens = 512\n",
    "    n_target_subtokens = target_subword_indices.count(True)\n",
    "    tokens_before = int((max_tokens - n_target_subtokens) * truncation_tokens_before_target)\n",
    "    tokens_after = max_tokens - tokens_before - n_target_subtokens\n",
    "\n",
    "    # get index of the first target subword\n",
    "    lindex_target = target_subword_indices.index(True)\n",
    "    # get index of the last target subword\n",
    "    rindex_target = lindex_target + n_target_subtokens\n",
    "    # starting index for truncation\n",
    "    lindex = max(lindex_target - tokens_before, 0)\n",
    "    # ending index for truncation\n",
    "    rindex = rindex_target + tokens_after\n",
    "    return lindex, rindex\n",
    "\n",
    "def get_target_token_embedding(context, index, truncation_tokens_before_target=0.5):\n",
    "    start_idx = int(str(index).strip().split(':')[0])\n",
    "    end_idx = int(str(index).strip().split(':')[1])\n",
    "\n",
    "    # tokenize the context with offset mapping\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=False).to(device)\n",
    "\n",
    "    # offset mapping to provide the start and end positions of each token in the original context\n",
    "    offset_mapping = inputs['offset_mapping'][0].tolist()\n",
    "\n",
    "    # convert input ids to tokens\n",
    "    input_ids = inputs['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # create a boolean mask for subwords within the target words span\n",
    "    subwords_bool_mask = [\n",
    "        (start <= start_idx < end) or (start < end_idx <= end) or (start_idx <= start and end <= end_idx)\n",
    "        for start, end in offset_mapping\n",
    "    ]\n",
    "\n",
    "    target_token_indices = [i for i, value in enumerate(subwords_bool_mask) if value]\n",
    "\n",
    "    if not target_token_indices:\n",
    "        print(\n",
    "            f\"Error: Target token indices not found within the specified range for context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "\n",
    "    # truncate input if it exceeds 512 tokens\n",
    "    if len(input_ids[0]) > 512:\n",
    "        # truncation indices based on the subwords boolean mask\n",
    "        lindex, rindex = truncation_indices(subwords_bool_mask, truncation_tokens_before_target)\n",
    "\n",
    "        # truncate the tokens, input_ids and subwords_bool_mask within the range of truncation indices\n",
    "        tokens = tokens[lindex:rindex]\n",
    "        input_ids = input_ids[:, lindex:rindex]\n",
    "        subwords_bool_mask = subwords_bool_mask[lindex:rindex]\n",
    "        offset_mapping = offset_mapping[lindex:rindex]\n",
    "        inputs['input_ids'] = input_ids  # update the input_ids in the inputs dictionary\n",
    "\n",
    "        # check if truncation was successful\n",
    "        if len(input_ids[0]) > 512:\n",
    "            print(\n",
    "                f\"Truncation failed: input seq len ({len(input_ids[0])}) exceeds the maximum token limit for context: '{context}' and index: '{index}'\")\n",
    "            return None\n",
    "\n",
    "    # extract the subwords for the target word\n",
    "    extracted_subwords = [tokens[i] for i, value in enumerate(subwords_bool_mask) if value]\n",
    "\n",
    "    if not extracted_subwords:\n",
    "        print(f\"Error: no subwords extracted for the target word in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs['input_ids'], output_hidden_states=True).hidden_states # get embeddings for the truncated input\n",
    "        # you can freely choose the layers of the model here\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "    # embeddings for all tokens in the truncated input\n",
    "    embeddings = outputs[0]\n",
    "\n",
    "    # embeddings for target token\n",
    "    target_embeddings = embeddings[subwords_bool_mask]\n",
    "\n",
    "    if target_embeddings.size(0) == 0:\n",
    "        print(f\"error: no embeddings found for the target token in context: '{context}' and index: '{index}'\")\n",
    "        return None\n",
    "\n",
    "    # aggregated target token embedding\n",
    "    target_embeddings_nump = target_embeddings.mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return target_embeddings_nump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_train_uses_merged, df_dev_uses_merged, df_test_uses_merged]\n",
    "file_names = ['subtask1_train_embeddings.npz', 'subtask1_dev_embeddings.npz', 'subtask1_test_embeddings.npz']\n",
    "\n",
    "# getting target token embeddings for contexts in train and dev\n",
    "for df, file_name in zip(dataframes, file_names):\n",
    "    id2embedding = {}\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        identifier1 = row['identifier1']\n",
    "        identifier2 = row['identifier2']\n",
    "\n",
    "        if identifier1 not in id2embedding:\n",
    "            embedding1 = get_target_token_embedding(row['context1'], row['index_target_token1'])\n",
    "            id2embedding[identifier1] = embedding1\n",
    "\n",
    "        if identifier2 not in id2embedding:\n",
    "            embedding2 = get_target_token_embedding(row['context2'], row['index_target_token2'])\n",
    "            id2embedding[identifier2] = embedding2\n",
    "\n",
    "    # store embeddings in a .npz file using identifiers as keys\n",
    "    np.savez(file_name, **id2embedding)\n",
    "\n",
    "dataframes = [df_train_uses_merged, df_dev_uses_merged, df_test_uses_merged]\n",
    "file_names = ['subtask1_train_embeddings.npz', 'subtask1_dev_embeddings.npz', 'subtask1_test_embeddings.npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize training set\n",
    "train_input_list = []\n",
    "train_language_list = []\n",
    "train_label_list = []\n",
    "train_ident1_list = []\n",
    "train_ident2_list = []\n",
    "loaded_train_embeddings = np.load('subtask1_train_embeddings.npz')\n",
    "for _, row in tqdm(df_train_uses_merged.iterrows()):\n",
    "    context_embedding1 = loaded_train_embeddings[row['identifier1']]\n",
    "    context_embedding2 = loaded_train_embeddings[row['identifier2']]\n",
    "    context_embedding1 = context_embedding1.reshape(1, 768)\n",
    "    context_embedding2 = context_embedding2.reshape(1, 768)\n",
    "    combined_matrix = np.concatenate((context_embedding1, context_embedding2), axis=1)\n",
    "    train_input_list.append(combined_matrix.tolist())\n",
    "    train_language_list.append(row['language'])\n",
    "    train_label_list.append(int(row['median_cleaned'][0])-1)\n",
    "    train_ident1_list.append(row['identifier1'])\n",
    "    train_ident2_list.append(row['identifier2'])\n",
    "\n",
    "# organize dev set\n",
    "dev_input_list = []\n",
    "dev_language_list = []\n",
    "dev_label_list = []\n",
    "dev_ident1_list = []\n",
    "dev_ident2_list = []\n",
    "loaded_dev_embeddings = np.load('subtask1_dev_embeddings.npz')\n",
    "for _, row in tqdm(df_dev_uses_merged.iterrows()):\n",
    "    context_embedding1 = loaded_dev_embeddings[row['identifier1']]\n",
    "    context_embedding2 = loaded_dev_embeddings[row['identifier2']]\n",
    "    context_embedding1 = context_embedding1.reshape(1, 768)\n",
    "    context_embedding2 = context_embedding2.reshape(1, 768)\n",
    "    combined_matrix = np.concatenate((context_embedding1, context_embedding2), axis=1)\n",
    "    dev_input_list.append(combined_matrix.tolist())\n",
    "    dev_language_list.append(row['language'])\n",
    "    dev_label_list.append(int(row['median_cleaned'][0])-1)\n",
    "    dev_ident1_list.append(row['identifier1'])\n",
    "    dev_ident2_list.append(row['identifier2'])\n",
    "\n",
    "# organize test set\n",
    "test_input_list = []\n",
    "test_language_list = []\n",
    "test_label_list = []\n",
    "test_ident1_list = []\n",
    "test_ident2_list = []\n",
    "loaded_test_embeddings = np.load('subtask1_test_embeddings.npz')\n",
    "for _, row in tqdm(df_test_uses_merged.iterrows()):\n",
    "    context_embedding1 = loaded_test_embeddings[row['identifier1']]\n",
    "    context_embedding2 = loaded_test_embeddings[row['identifier2']]\n",
    "    context_embedding1 = context_embedding1.reshape(1, 768)\n",
    "    context_embedding2 = context_embedding2.reshape(1, 768)\n",
    "    combined_matrix = np.concatenate((context_embedding1, context_embedding2), axis=1)\n",
    "    test_input_list.append(combined_matrix.tolist())\n",
    "    test_language_list.append(row['language'])\n",
    "    test_label_list.append(int(row['median_cleaned'][0])-1)\n",
    "    test_ident1_list.append(row['identifier1'])\n",
    "    test_ident2_list.append(row['identifier2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some processing of the dataset before putting it into the model\n",
    "def trans2TheDataset(data):\n",
    "    inputs = list()\n",
    "    languages = list()\n",
    "    labels = list()\n",
    "    ident_all1 = list()\n",
    "    ident_all2 = list()\n",
    "    for input, language, label, ident1, ident2 in data:\n",
    "        inputs.append(input)\n",
    "        languages.append(language)\n",
    "        labels.append(label)\n",
    "        ident_all1.append(ident1)\n",
    "        ident_all2.append(ident2)\n",
    "    inputs = torch.tensor(inputs).to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "    return inputs, languages, labels, ident_all1, ident_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random number seed\n",
    "def seedConfig(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, all_input, all_language, all_label, all_ident1, all_ident2):\n",
    "        self.all_input = all_input\n",
    "        self.all_language = all_language\n",
    "        self.all_label = all_label\n",
    "        self.all_ident1 = all_ident1\n",
    "        self.all_ident2 = all_ident2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.all_input[index]\n",
    "        language = self.all_language[index]\n",
    "        label = self.all_label[index]\n",
    "        ident1 = self.all_ident1[index]\n",
    "        ident2 = self.all_ident2[index]\n",
    "        return input, language, label, ident1, ident2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP model\n",
    "class MModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MModel, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.trans = nn.Linear(768 * 2, 768) # linear layers used to reduce dimensionality\n",
    "        self.classifier = nn.Linear(768, num_labels) # linear layer used for mapping to classification labels\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_vector = self.trans(inputs.squeeze(1))\n",
    "        hidden_vector = torch.relu(hidden_vector)\n",
    "        hidden_vector = self.dropout(hidden_vector)\n",
    "        logits = self.classifier(hidden_vector) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_loop(train_dataloader, model, loss_fn, optimizer, epoch):\n",
    "    total_loss_train = 0\n",
    "    correct = 0\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for inputs, languages, labels, ident1, ident2 in tqdm(train_dataloader, colour='green'):\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        lr_scheduler.step()  \n",
    "        correct += (logits.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    correct /= size\n",
    "    print(f'Epoch {epoch} Train Acc: {(100 * correct):>0.6f}%   Loss: {total_loss_train / size:.5f}', flush=True)\n",
    "    return total_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev process (also test process)\n",
    "def valid_loop(dataloader, model, epoch, record):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for inputs, languages, labels, ident1, ident2 in tqdm(dataloader, colour='blue'):\n",
    "        logits = model(inputs)\n",
    "        logits_softmax = torch.softmax(logits, dim=1).tolist()\n",
    "        pre_list = logits.argmax(1).tolist()\n",
    "        pre_list_median = []\n",
    "        for pre in pre_list:\n",
    "            pre_list_median.append(str(pre + 1) + '.0')\n",
    "        for ide1, ide2, language, pre, pre_logits in zip(ident1, ident2, languages, pre_list_median, logits_softmax):\n",
    "            record[language]['ident1'].append(ide1)\n",
    "            record[language]['ident2'].append(ide2)\n",
    "            record[language]['prediction'].append(pre)\n",
    "            record[language]['prob_max'].append(max(pre_logits))\n",
    "            record[language]['prob_0'].append(pre_logits[0])\n",
    "            record[language]['prob_1'].append(pre_logits[1])\n",
    "            record[language]['prob_2'].append(pre_logits[2])\n",
    "            record[language]['prob_3'].append(pre_logits[3])\n",
    "\n",
    "        correct += (logits.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    correct /= size\n",
    "    print(f'Epoch {epoch} Valid Acc: {(100 * correct):>0.6f}% ', flush=True)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_krippendorff(path_pre: str, path_true: str):\n",
    "    # as per the metadata file, input and output directories are the arguments  \n",
    "    [_, input_dir1, input_dir2] = sys.argv, path_pre, path_true\n",
    "\n",
    "    languages = ['chinese', 'german', 'english', 'norwegian', 'russian', 'spanish', 'swedish']\n",
    "    columns = ['SCORE_ALL', 'SCORE_CHINESE', 'SCORE_ENGLISH', 'SCORE_GERMAN', 'SCORE_NORWEGIAN', 'SCORE_RUSSIAN',\n",
    "               'SCORE_SPANISH', 'SCORE_SWEDISH']\n",
    "\n",
    "    language2column = {'average': 'SCORE_AVERAGE', 'chinese': 'SCORE_CHINESE', 'english': 'SCORE_ENGLISH',\n",
    "                       'german': 'SCORE_GERMAN', 'norwegian': 'SCORE_NORWEGIAN', 'russian': 'SCORE_RUSSIAN',\n",
    "                       'spanish': 'SCORE_SPANISH', 'swedish': 'SCORE_SWEDISH'}\n",
    "\n",
    "    scores = {}\n",
    "    for language in languages:\n",
    "        # Load submission file\n",
    "        submission_file_name = language + '.tsv'\n",
    "        # submission_dir = os.path.join(input_dir, 'res')\n",
    "        # submission_path = os.path.join(submission_dir, submission_file_name)\n",
    "        submission_path = os.path.join(input_dir1, submission_file_name)\n",
    "        # if not os.path.exists(submission_path):\n",
    "        #     message = \"Error: Expected submission file '{0}', found files {1}\"\n",
    "        #     sys.exit(message.format(submission_file_name, os.listdir(submission_dir)))\n",
    "\n",
    "        submission = {}\n",
    "        with open(submission_path, mode='r') as submission_file:\n",
    "            reader = csv.DictReader(submission_file, delimiter='\\t', quoting=csv.QUOTE_NONE, strict=True)\n",
    "            for row in reader:\n",
    "                key = (row['identifier1'], row['identifier2'])\n",
    "                submission[key] = float(row['prediction'])\n",
    "\n",
    "        # Load truth file\n",
    "        truth_file_name = language + '/labels.tsv'\n",
    "        # truth_dir = os.path.join(input_dir, 'ref')\n",
    "        # truth_path = os.path.join(truth_dir, truth_file_name)\n",
    "        truth_path = os.path.join(input_dir2, truth_file_name)\n",
    "        # if not os.path.exists(truth_path):\n",
    "        #     message = \"Error: Expected truth file '{0}', found files {1}\"\n",
    "        #     sys.exit(message.format(truth_file_name, os.listdir(truth_dir)))\n",
    "\n",
    "        truth = {}\n",
    "        with open(truth_path, mode='r') as truth_file:\n",
    "            reader = csv.DictReader(truth_file, delimiter='\\t', quoting=csv.QUOTE_NONE, strict=True)\n",
    "            for row in reader:\n",
    "                key = (row['identifier1'], row['identifier2'])\n",
    "                truth[key] = float(row['median_cleaned'])\n",
    "\n",
    "        # Check submission format\n",
    "        if set(submission.keys()) != set(truth.keys()) or len(submission.keys()) != len(truth.keys()):\n",
    "            message = \"Error in '{0}': Submitted targets do not match gold targets.\"\n",
    "            sys.exit(message.format(truth_path))\n",
    "\n",
    "        if any((not (i == 1.0 or i == 2.0 or i == 3.0 or i == 4.0) for i in truth.values())):\n",
    "            message = \"Error in '{0}': Submitted values contain values that are not equal to ordinal label range.\"\n",
    "            sys.exit(message.format(truth_path))\n",
    "\n",
    "        # Get submitted values and true values\n",
    "        submission_values = [submission[target] for target in truth.keys()]\n",
    "        truth_values = [truth[target] for target in truth.keys()]\n",
    "\n",
    "        # Calculate score\n",
    "        data = [truth_values, submission_values]\n",
    "        # print(truth_values)\n",
    "        score = krippendorff.alpha(reliability_data=data, level_of_measurement=\"ordinal\")\n",
    "        scores[language] = score\n",
    "\n",
    "    # Calculate the average score\n",
    "    average_score = np.mean([scores[language] for language in languages])\n",
    "    scores['average'] = average_score\n",
    "\n",
    "    return average_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # file path\n",
    "    current_directory = os.path.dirname(__file__)\n",
    "    output_dir = current_directory + '/result/'\n",
    "\n",
    "    # set random number seed\n",
    "    seed = 1\n",
    "    seedConfig(seed)\n",
    "\n",
    "    # the hyperparameters required for training\n",
    "    learning_rate = 1e-2\n",
    "    epoch_num = 50\n",
    "    batch_size = 128\n",
    "    num_labels = 4\n",
    "\n",
    "    # organize the dataset\n",
    "    train_dataset = TextDataset(train_input_list, train_language_list, train_label_list, train_ident1_list, train_ident2_list)\n",
    "    valid_dataset = TextDataset(dev_input_list, dev_language_list, dev_label_list, dev_ident1_list, dev_ident2_list)\n",
    "    test_dataset = TextDataset(test_input_list, test_language_list, test_label_list, test_ident1_list, test_ident2_list)\n",
    "\n",
    "    # create data loader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=trans2TheDataset)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size, shuffle=False, collate_fn=trans2TheDataset)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=trans2TheDataset)\n",
    "\n",
    "    # instantiate model\n",
    "    model = MModel(num_labels).to(device)\n",
    "\n",
    "    # define loss function, optimizer and lr_scheduler\n",
    "    # weighted cross entropy loss function\n",
    "    # weights = torch.tensor([4.3, 6.7, 5.0, 1.0]).to(device)\n",
    "    # loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    classifier_optimizer = list(model.classifier.named_parameters())\n",
    "    trans_optimizer = list(model.trans.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in trans_optimizer if not any(nd in n for nd in no_decay)],  \n",
    "         'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in trans_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in classifier_optimizer if not any(nd in n for nd in no_decay)],  \n",
    "         'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in classifier_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                                 num_warmup_steps=epoch_num * len(train_dataloader) * 0.1,\n",
    "                                 num_training_steps=epoch_num * len(train_dataloader))\n",
    "    \n",
    "    # record the effectiveness of each epoch during the training process\n",
    "    train_loss = list()\n",
    "\n",
    "    valid_acc = list()\n",
    "    valid_k_avg = []\n",
    "    valid_k_all = []\n",
    "\n",
    "    test_acc = list()\n",
    "    test_k_avg = []\n",
    "    test_k_all = []\n",
    "\n",
    "    print(f'*************** Training begins, learning rate of {learning_rate}, batch size of {batch_size}, epoch of {epoch_num}, and random seed number of {seed} ****************')\n",
    "    for t in range(epoch_num):\n",
    "        print(f'Epoch {t + 1}/{epoch_num}\\n-----------------------', flush=True)\n",
    "        # start model training\n",
    "        loss = train_loop(train_dataloader, model, loss_fn, optimizer, t + 1)\n",
    "        train_loss.append(loss)\n",
    "        # save validation set results\n",
    "        record = {'chinese': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'german': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                             'prob_2': [], 'prob_3': []},\n",
    "                  'english': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'norwegian': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                             'prob_2': [], 'prob_3': []},\n",
    "                  'russian': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'spanish': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'swedish': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []}\n",
    "                }\n",
    "\n",
    "        acc_dev = valid_loop(valid_dataloader, model, t + 1, record)\n",
    "        valid_acc.append(acc_dev)\n",
    "\n",
    "        # save the results of each epoch in the required evaluation format\n",
    "        valid_out_dir = output_dir + f'dev/{t + 1}/'\n",
    "        if not os.path.exists(valid_out_dir):\n",
    "            os.makedirs(valid_out_dir)\n",
    "        for language in record:\n",
    "            df = pd.DataFrame({'identifier1': record[language]['ident1'], 'identifier2': record[language]['ident2'],\n",
    "                               'prediction': record[language]['prediction'],\n",
    "                               'prob_max': record[language]['prob_max'], 'prob_0': record[language]['prob_0'],\n",
    "                               'prob_1': record[language]['prob_1'], 'prob_2': record[language]['prob_2'],\n",
    "                               'prob_3': record[language]['prob_3']})\n",
    "            df.to_csv(valid_out_dir + language + '.tsv', index=False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "        # calculate krippendorff on the validation set\n",
    "        k_dev_avg, k_dev_all = get_krippendorff(valid_out_dir, path_dev)\n",
    "        print('***************', 'avg krippendorff on the validation set: ', k_dev_avg, '***************')\n",
    "        valid_k_avg.append(k_dev_avg)\n",
    "        valid_k_all.append((k_dev_all))\n",
    "\n",
    "        # save test set results\n",
    "        record = {'chinese': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'german': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                             'prob_2': [], 'prob_3': []},\n",
    "                  'english': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'norwegian': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'russian': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'spanish': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []},\n",
    "                  'swedish': {'ident1': [], 'ident2': [], 'prediction': [], 'prob_max': [], 'prob_0': [], 'prob_1': [],\n",
    "                              'prob_2': [], 'prob_3': []}\n",
    "                  }\n",
    "\n",
    "        acc_test = valid_loop(test_dataloader, model, t + 1, record)\n",
    "        test_acc.append(acc_test)\n",
    "\n",
    "        # save the results of each epoch in the required evaluation format\n",
    "        test_out_dir = output_dir + f'test/{t + 1}/'\n",
    "        if not os.path.exists(test_out_dir):\n",
    "            os.makedirs(test_out_dir)\n",
    "        for language in record:\n",
    "            df = pd.DataFrame({'identifier1': record[language]['ident1'], 'identifier2': record[language]['ident2'],\n",
    "                               'prediction': record[language]['prediction'],\n",
    "                               'prob_max': record[language]['prob_max'], 'prob_0': record[language]['prob_0'],\n",
    "                               'prob_1': record[language]['prob_1'], 'prob_2': record[language]['prob_2'],\n",
    "                               'prob_3': record[language]['prob_3']})\n",
    "            df.to_csv(test_out_dir + language + '.tsv', index=False, sep='\\t', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "\n",
    "        # calculate krippendorff on the validation set\n",
    "        k_test_avg, k_test_all = get_krippendorff(test_out_dir, path_test)\n",
    "        print('***************', 'avg krippendorff on the test set: ', k_dev_avg, '***************')\n",
    "        test_k_avg.append(k_test_avg)\n",
    "        test_k_all.append(k_test_all)\n",
    "\n",
    "    print(f'*************** Training ends, learning rate of {learning_rate}, batch size of {batch_size}, epoch of {epoch_num}, and random seed number of {seed} ****************')\n",
    "    print('loss for each epoch during the training process: ', train_loss)\n",
    "    print('avg krippendorff on the validation set: ', valid_k_avg)\n",
    "    # select the epoch corresponding to the highest krippendorff on the validation set\n",
    "    best_valid_k_avg = max(valid_k_avg)\n",
    "    best_epoch_k_avg = valid_k_avg.index(best_valid_k_avg)\n",
    "    best_test_k_avg = test_k_avg[best_epoch_k_avg]\n",
    "    best_valid_k_all = valid_k_all[best_epoch_k_avg]\n",
    "    best_test_k_all = test_k_all[best_epoch_k_avg]\n",
    "\n",
    "    print(f'*************** Select the {best_epoch_k_avg + 1} epoch ****************')\n",
    "    print(f'*************** The krippendorff on the validation set is {best_valid_k_all} ****************')\n",
    "    print(f'*************** The krippendorff on the test set is {best_valid_k_all} ****************')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
